  <!------------------------------------------------------------------------------DOCUMENT CONFIGURATION----------------------------------------------------------------------------->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="From Pixels to Wireframes: 3D Reconstruction via CLIP-Based Sketch Abstraction.">
  <meta name="keywords" content="3D Sketches, Wireframes, 3D Vision, CLIP, CLIPasso">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>From Pixels to Wireframes: 3D Reconstruction via CLIP-Based Sketch Abstraction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>-->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<!------------------------------------------------------------------------------NAVIGATION BAR BUTTONS----------------------------------------------------------------------------->


<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://vilab.epfl.ch/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>

<!------------------------------------------------------------------------------AUTHORS SECTION----------------------------------------------------------------------------->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">From Pixels to Wireframes: 3D Reconstruction via CLIP-Based Sketch Abstraction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tarhanefe.github.io/">Efe Tarhan</a>,</span>
            <span class="author-block">
              <a href="https://people.epfl.ch/filip.mikoviny/?lang=en">Filip Mikovíny</a>,</span>
            <span class="author-block">
              <a href="https://people.epfl.ch/eylul.ipci?lang=en">Eylül Ipçi</a>,</span>
            <span class="author-block">
              <a href="https://people.epfl.ch/alberts.reisons">Alberts Reisons</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Swiss Federal Institute of Technology Lausanne (EPFL)</span>
          </div>
  
<!------------------------------------------------------------------------------LINKS UNDER AUTHORS----------------------------------------------------------------------------->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tarhanefe/clipasso3d"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/Qa8kh6aMo-8"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/tarhanefe/CLIPasso3DWebsite"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Website</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!------------------------------------------------------------------------------TEASER GIF----------------------------------------------------------------------------->


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="display: flex; justify-content: center; gap: 1rem;">
        <img src="./static/gifs/rose_gt.gif" alt="Ground Truth Rose" style="max-width: 100%; height: auto;">
        <img src="./static/gifs/rose_20.gif" alt="3D Abstract Sketch of Rose" style="max-width: 100%; height: auto;">
      </div>
    </div>
  </div>
</section>


<!------------------------------------------------------------------------------ABSTRACT----------------------------------------------------------------------------->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Sketch abstraction allows objects to be represented with minimal yet meaningful visual elements. 
            While recent methods enable machines to generate 2D sketches from images, they remain limited to flat representations. 
            In this project, we extend sketch abstraction to 3D by optimizing Bézier curves on reconstructed surfaces, using CLIP-based losses to produce semantically meaningful and view-consistent wireframe sketches.
          </p>
        </div>

      </div>
    </div>

<!------------------------------------------------------------------------------CONTENT TABLE----------------------------------------------------------------------------->

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">

        <h2 class="title is-3">Overview</h2>
        <div class="tile is-ancestor is-centered" style="vertical-align: middle;">
          <div class="tile is-parent">
            <a href="#intro" class="tile is-child box">
              <p class="subtitle"><font size="-1">Introduction</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#related-work" class="tile is-child box">
              <p class="subtitle"><font size="-1">Related Works</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#method" class="tile is-child box">
              <p class="subtitle"><font size="-1">Methodology</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#exp" class="tile is-child box">
              <p class="subtitle"><font size="-1">Experiments</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#conclusion" class="tile is-child box">
              <p class="subtitle"><font size="-1">Conclusion and Limitations</font></p>
            </a>
          </div>
        </div>

      </div>
    </div>
    <!--/ Abstract. -->


  <!------------------------------------------------------------------------------PROJECT VIDEO---------------------------------------------------------------------------->

    <!-- Paper video.
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="project-video">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/Qa8kh6aMo-8?si=Clq5caeAaY32thoL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    Paper video. -->
  </div>
</section>



  <!------------------------------------------------------------------------------INTRODUCTION----------------------------------------------------------------------------->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="intro">I. Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Sketching is a powerful form of abstraction that captures the essence of objects using minimal visual elements. 
            This process requires selecting key visual features that convey meaning while omitting others, which demands a level of semantic understanding that is more difficult for machines than for humans. 
            Recent advances, such as CLIPasso \cite{vinker2022clipasso}, have enabled machines to automatically convert images into recognizable abstract sketches using semantic information utilizing SOTA image-text representation method Contrastive Language–Image Pretraining (CLIP) \cite{CLIP}. 
            However, these methods are currently limited to two dimensions. 
          </p>
          <p>
            In this project, we try to extend this idea to 3D: can machines generate 3D sketches that preserve both semantic and geometric information of an object? 
            Addressing this question could open up new possibilities for visual communication and rapid prototyping, paving the way for innovative tools in 3D design and visualization.
          </p>
        </div>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>



<!------------------------------------------------------------------------------RELATED WORK----------------------------------------------------------------------------->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Related work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="related-work">II. Related Works</h2>
        <div class="content has-text-justified">
          <p>
            The intersection of 3D sketch abstraction and differentiable rendering is a vibrant area of research. Our project builds directly on two complementary threads: semantically-driven sketch generation using CLIP, and efficient, real-time rendering through Gaussian primitives. Below, we outline key prior work in both directions that inform our method.
          </p>
        </div>
      </div>
    </div>
  </div>
  <br>

  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- CLIPasso -->
      <div class="column">
        <div class="content">
          <h2 class="title is-6">CLIPasso: Semantically-Aware Object Sketching
                                 <a href="https://clipasso.github.io/clipasso/">[1]</a></h2>
          <p>
            CLIPasso introduced a novel sketch abstraction pipeline driven by CLIP embeddings. It optimizes 2D Bézier curves to semantically match target images, generating vector sketches that maintain both recognizability and abstraction. CLIP-based supervision enables sketching without pixel-wise supervision, focusing instead on perceptual similarity.
          </p>
          <div class="box">
            <img src="static/website_assets/related_1.gif"
                 alt="CLIPasso vector sketch abstraction.">
            <h5 class="subtitle has-text-centered">
              <font size="-1.5">
                <span style="font-weight:normal">Sketches of objects obtained from 2D images using CLIPasso.</span>
              </font>
            </h5>
          </div>
          <p>
            However, CLIPasso operates purely in 2D and does not generalize to multiview-consistent or 3D-aware sketching tasks.
          </p>
        </div>
      </div>
      <!--/ CLIPasso -->

      <!-- Gaussian Splatting -->
      <div class="column">
        <div class="content">
          <h2 class="title is-6">3D Gaussian Splatting for Real-Time Radiance Field Rendering
                                 <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">[2]</a></h2>
          <p>
            This work replaces dense volumetric representations with a sparse set of view-adaptive anisotropic Gaussians, achieving high-quality and real-time radiance field rendering. The technique offers a fully differentiable rendering pipeline ideal for interactive and fast applications.
          </p>
          <div class="box">
            <img src="static/website_assets/related_2.gif"
                 alt="CLIPasso vector sketch abstraction.">
            <h5 class="subtitle has-text-centered">
              <font size="-1.5">
                <span style="font-weight:normal">Gaussian Splatting pipeline explained.</span>
              </font>
            </h5>
          </div>
          <p>
            While highly efficient, this work focuses on photorealistic synthesis and lacks semantic constraints. Our work adapts its rasterization foundation for use with sparse Bézier-based sketches guided by perceptual objectives.
          </p>
        </div>
      </div>
    </div>
  </div>
  <br>

  <div class="container is-max-desktop">
    <!-- Conclusion of Related Work -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Combining the semantic sketching approach of CLIPasso with the real-time rendering capabilities of Gaussian Splatting, our framework introduces a differentiable 3D sketch representation optimized using CLIP-based supervision. This positions our work uniquely at the intersection of 3D vision, neural rendering, and semantic abstraction.
          </p>
        </div>
      </div>
    </div>
    <!--/ Conclusion -->
  </div>
</section>


  
<!------------------------------------------------------------------------------METHODOLOGY----------------------------------------------------------------------------->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="method">III. Methodology</h2>
        <div class="content has-text-justified">
          <p>
            In this section, we present the methodology utilized for this paper. We first discuss the environment setup,
            after which we detail how the predator and prey agents are created. Finally, we talk about how the agents are optimized.
          </p>
          <p>
            We create a 3D environment using the Unity game development engine <a href="https://unity.com/">[3]</a>. The selection of Unity as the engine
            for creating the environment instead of a different simulator is motivated by several key factors. Mainly, Unity's ML-Agents
            package <a href="https://arxiv.org/abs/1809.02627">[4]</a> offers support for the OpenAI Gym framework
            <a href="https://arxiv.org/abs/1809.02627">[5]</a>, which is widely used in
            reinforcement learning research. This way, Unity enables interaction between the environment and the learning algorithms,
            allowing for efficient experimentation and evaluation of predator-prey agents.
          </p>
          <p>
            Moreover, the engine offers a wide range of built-in tools, such as physics simulations, ray tracing, and collision.
            Finally, Unity provides a user-friendly and intuitive development environment, making it accessible to researchers with
             varying levels of expertise.
          </p>
        </div>
        <!-- Creating environments. -->

        
<!------------------------------------------------------------------------------SPHERICAL GAUSSIANS----------------------------------------------------------------------------->
        
<h3 class="title is-4">III-A. 3D Differentiable Rasterization with Spherical Gaussians</h3>
<div class="content has-text-justified">
  <p>
    To represent 3D sketches in a differentiable manner, we model 3D Bézier curves using spherical Gaussians (SGs) sampled along each curve’s path. This design allows efficient rasterization of the sketches into 2D images for training. Unlike CLIPasso’s discrete 2D rasterization, our approach supports backpropagation in 3D space and enables full differentiability with respect to the control points. Inspired by recent advances in Gaussian Splatting, we represent Bézier curves as sequences of Gaussians, each defined by a center and a fixed thickness, that can be projected onto 2D views using differentiable camera models.
  </p>

  <div class="columns is-centered">
    <div class="column is-three-quarters">
      <div align="center" class="box">
        <img src="static/website_assets/image_2.png" alt="3D Bezier curves are used for representing 3D sketches of objects.">
        <h5 class="subtitle has-text-centered">
          <font size="-0.7">
            <span style="font-weight:normal">
              3D Bézier curves are used for representing 3D sketches of objects.
            </span>
          </font>
        </h5>
      </div>
    </div>
  </div>

  <p>
    Each spherical Gaussian is defined as:
  </p>
  <p>
    <script type="math/tex; mode=display">
      \phi\left(\mathbf{x}; \mathbf{c}, r\right) = \exp\left(-\frac{\|\mathbf{x} - \mathbf{c}\|^2}{2r^2}\right)
    </script>
  </p>

  <p>
    where <script type="math/tex">\mathbf{c} \in \mathbb{R}^3</script> is the center of the Gaussian and <script type="math/tex">r</script> is its radius (or thickness). To sample a Bézier curve of length <script type="math/tex">L</script> with a desired overlap ratio <script type="math/tex">\alpha_o</script>, the sampling step <script type="math/tex">d</script> and the number of samples <script type="math/tex">N</script> are computed as:
  </p>

  <p>
    <script type="math/tex; mode=display">
      d = r \alpha_o, \quad
      N = \left\lceil \frac{L}{d} \right\rceil
    </script>
  </p>

  <p>
    For a Bézier curve <script type="math/tex">\mathbf{B}</script> with control points <script type="math/tex">\mathbf{p}_0, \mathbf{p}_1, \ldots, \mathbf{p}_M \in \mathbb{R}^3</script>, the centers of the Gaussians sampled along the curve are given by:
  </p>

  <p>
    <script type="math/tex; mode=display">
      \mathbf{c}_{n,\mathbf{B}} =
      \sum_{i=0}^{M}
      \gamma_i\,\left(1-\frac{n}{N}\right)^{M-i}\,\left(\frac{n}{N}\right)^{i}\,\mathbf{p}_i
    </script>
  </p>

  <p>
    where <script type="math/tex">\gamma_i</script> are the Bernstein basis coefficients. This makes each Gaussian center fully differentiable with respect to the Bézier control points.
  </p>

  <div class="columns is-centered">
    <div class="column is-three-quarters">
      <div align="center" class="box">
        <img src="static/website_assets/image_3.png" alt="Spherical Gaussians are utilized for differentiable rasterization of the 3D sketches.">
        <h5 class="subtitle has-text-centered">
          <font size="-0.7">
            <span style="font-weight:normal">
              Spherical Gaussians are utilized for differentiable rasterization of the 3D sketches.
            </span>
          </font>
        </h5>
      </div>
    </div>
  </div>

  <p>
    After generating the Gaussians, we render them through a differentiable rasterizer that projects the 3D Gaussians into 2D using camera intrinsics and extrinsics. The resulting image <script type="math/tex">\mathbf{V} \in \mathbb{R}^{W \times H}</script> is given by:
  </p>

  <p>
    <script type="math/tex; mode=display">
      \mathbf{V} = T\left(\mathbf{c}, r, \mathbf{In}, \mathbf{Ex}, W, H\right)
    </script>
  </p>

  <p>
    where <script type="math/tex">\mathbf{In}</script> and <script type="math/tex">\mathbf{Ex}</script> denote the intrinsic and extrinsic camera matrices, and <script type="math/tex">W, H</script> define the image resolution. Each pixel <script type="math/tex">\mathbf{V}_{n_1,n_2}</script> can be written as:
  </p>

  <p>
    <script type="math/tex; mode=display">
      \mathbf{V}_{n_1,n_2} = T\left(f(\mathbf{p}_i, \ldots), \ldots\right)
    </script>
  </p>

  <p>
    This end-to-end differentiability allows training the sketch representation directly using image-level loss functions such as CLIP similarity. It was one of the key milestones proposed for the project.
  </p>
</div>
        <!--/ Creating environments. -->

<!------------------------------------------------------------------------------CLIP LOSS----------------------------------------------------------------------------->

  <h3 class="title is-4">III-B. CLIP-based Objective</h3>
<div class="content has-text-justified">
  <p>
    To guide the optimization of the 3D Bézier sketch toward both geometrically accurate and semantically meaningful representations, we leverage a CLIP-based loss function. This loss encourages the rendered sketch views to match the real images not just pixel-wise but in high-level perceptual and semantic space. Following the strategy of CLIPasso, we define a dual-component objective that combines semantic similarity and geometric alignment, each captured from different layers of a pretrained CLIP model.
  </p>

  <p>
    The final loss is a weighted sum of the semantic and geometric components:
  </p>

  <p>
    <script type="math/tex; mode=display">
      \mathcal{L} = w_s \,\mathcal{L}_{\mathrm{semantic}}
           + w_g \,\mathcal{L}_{\mathrm{geometric}}
    </script>
  </p>

  <p>
    Here, <script type="math/tex">w_s</script> and <script type="math/tex">w_g</script> are hyperparameters that balance the contribution of each term. The semantic loss <script type="math/tex">\mathcal{L}_{\mathrm{semantic}}</script> is computed using the output of CLIP’s final fully connected layer, which captures the global alignment between rendered sketches and their corresponding RGB images in CLIP’s joint vision-language embedding space.
  </p>

  <p>
    The geometric loss <script type="math/tex">\mathcal{L}_{\mathrm{geometric}}</script> captures local spatial structure and can be extracted either from CLIP’s early convolutional layers or from token-wise attention maps, depending on the selected variant. This term ensures that the layout and fine-scale details of the projected sketches align with real-world object contours.
  </p>

  <p>
    During training, the 3D sketch is projected into 2D using the differentiable rasterizer described in Section III-A. The resulting image is then passed through CLIP alongside its target RGB counterpart. The cosine distance between their embeddings drives the optimization, gradually refining the control points of the Bézier curves so that the rendered sketches become increasingly perceptually and semantically aligned with real views.
  </p>
</div>
<!------------------------------------------------------------------------------TRAINING----------------------------------------------------------------------------->
        <h3 class="title is-4">III-C. Model Training</h3>
        <div class="content has-text-justified">
          <p>
            The training phase aims to optimize a set of 3D Bézier curves such that their 2D projections resemble real object images when viewed from different camera angles. This is achieved through an iterative loop involving projection, perceptual comparison, and gradient-based refinement. By leveraging CLIP embeddings as a supervisory signal, the model learns to align the rendered sketch views with the semantics of real RGB views. The training continues until convergence, producing a geometry-aware sketch representation that is both visually and semantically faithful.
          </p>
            <div class="columns is-centered">
            <div class="column is-three-quarters">
              <div align="center" class="box">
                <img src="static/website_assets/method_1.gif" alt="Rasterization of images using camera matrix from the constructred 3D sketch.">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Rasterization of images using camera matrix from the constructred 3D sketch.</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>
          
          <p>
In each training iteration, the current 3D Bézier sketch is projected into 2D using the known camera intrinsics and extrinsics. This rasterization step generates a view-dependent binary sketch image that mimics how the 3D curves would appear from that specific viewpoint.          
          </p>
          
          <div class="columns is-centered">
            <div class="column is-three-quarters">
              <div align="center" class="box">
                <img src="static/website_assets/method_2.gif" alt="Obtaining the CLIP Loss between the RGB images of the views and sampled sketch views.">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Obtaining the CLIP Loss between the RGB images of the views and sampled sketch views.</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>

          <p>
  The projected sketch view is then compared to the corresponding RGB image of the same view. Both are encoded using CLIP’s vision-language model, and a cosine similarity loss is computed to quantify their perceptual alignment in the embedding space.
          </p>

          <div class="columns is-centered">
            <div class="column is-three-quarters">
              <div align="center" class="box">
                <img src="static/website_assets/method_3.gif" alt="Optimizing the control points using the backpropagated loss.">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Optimizing the control points using the backpropagated loss.</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>

          <p>
  The CLIP loss is backpropagated through the differentiable projection and rendering pipeline to adjust the 3D control points of the Bézier curves. This step ensures that the curves evolve to better match the visual content of the reference images.
          </p>

          <div class="columns is-centered">
            <div class="column is-three-quarters">
              <div align="center" class="box">
                <img src="static/website_assets/method_4.gif" alt="Training the model until loss converges.">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Training the model until loss converges.</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>

           <p>
  This process is repeated iteratively for all views in the dataset. Over time, the loss steadily decreases as the curves converge to a semantically accurate and geometrically coherent representation of the object across all viewpoints.
          </p>

          
        </div>
        <!--/ Creating environments. -->
  <!------------------------------------------------------------------------------TRAINING----------------------------------------------------------------------------->

      </div>
    </div>
  </div>
</section>






<!------------------------------------------------------------------------------EXPERIMENTS----------------------------------------------------------------------------->

  



<!------------------------------------------------------------------------------CONCLUSION & LIMITATIONS---------------------------------------------------------------->

  


  
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
