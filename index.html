  <!------------------------------------------------------------------------------DOCUMENT CONFIGURATION----------------------------------------------------------------------------->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="From Pixels to Wireframes: 3D Reconstruction via CLIP-Based Sketch Abstraction.">
  <meta name="keywords" content="3D Sketches, Wireframes, 3D Vision, CLIP, CLIPasso">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>From Pixels to Wireframes: 3D Reconstruction via CLIP-Based Sketch Abstraction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>-->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<!------------------------------------------------------------------------------NAVIGATION BAR BUTTONS----------------------------------------------------------------------------->


<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://vilab.epfl.ch/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>

<!------------------------------------------------------------------------------AUTHORS SECTION----------------------------------------------------------------------------->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">From Pixels to Wireframes: 3D Reconstruction via CLIP-Based Sketch Abstraction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tarhanefe.github.io/">Efe Tarhan</a>,</span>
            <span class="author-block">
              <a href="https://people.epfl.ch/filip.mikoviny/?lang=en">Filip Mikovíny</a>,</span>
            <span class="author-block">
              <a href="https://people.epfl.ch/eylul.ipci?lang=en">Eylül Ipçi</a>,</span>
            <span class="author-block">
              <a href="https://people.epfl.ch/alberts.reisons">Alberts Reisons</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Swiss Federal Institute of Technology Lausanne (EPFL)</span>
          </div>
  
<!------------------------------------------------------------------------------LINKS UNDER AUTHORS----------------------------------------------------------------------------->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tarhanefe/clipasso3d"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/Qa8kh6aMo-8"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/tarhanefe/CLIPasso3DWebsite"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Website</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!------------------------------------------------------------------------------TEASER GIF----------------------------------------------------------------------------->


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="display: flex; justify-content: center; gap: 1rem;">
        <img src="./static/gifs/rose_gt.gif" alt="Ground Truth Rose" style="max-width: 100%; height: auto;">
        <img src="./static/gifs/rose_20.gif" alt="3D Abstract Sketch of Rose" style="max-width: 100%; height: auto;">
      </div>
    </div>
  </div>
</section>


<!------------------------------------------------------------------------------ABSTRACT----------------------------------------------------------------------------->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Sketch abstraction allows objects to be represented with minimal yet meaningful visual elements. 
            While recent methods enable machines to generate 2D sketches from images, they remain limited to flat representations. 
            In this project, we extend sketch abstraction to 3D by optimizing Bézier curves on reconstructed surfaces, using CLIP-based losses to produce semantically meaningful and view-consistent wireframe sketches.
          </p>
        </div>

      </div>
    </div>

<!------------------------------------------------------------------------------CONTENT TABLE----------------------------------------------------------------------------->

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">

        <h2 class="title is-3">Overview</h2>
        <div class="tile is-ancestor is-centered" style="vertical-align: middle;">
          <div class="tile is-parent">
            <a href="#intro" class="tile is-child box">
              <p class="subtitle"><font size="-1">Introduction</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#related-work" class="tile is-child box">
              <p class="subtitle"><font size="-1">Related Works</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#method" class="tile is-child box">
              <p class="subtitle"><font size="-1">Methodology</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#results" class="tile is-child box">
              <p class="subtitle"><font size="-1">Results</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#conclusion" class="tile is-child box">
              <p class="subtitle"><font size="-1">Conclusion and Limitations</font></p>
            </a>
          </div>
        </div>

      </div>
    </div>
    <!--/ Abstract. -->


  <!------------------------------------------------------------------------------PROJECT VIDEO---------------------------------------------------------------------------->

    <!-- Paper video.
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="project-video">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/Qa8kh6aMo-8?si=Clq5caeAaY32thoL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    Paper video. -->
  </div>
</section>



  <!------------------------------------------------------------------------------INTRODUCTION----------------------------------------------------------------------------->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="intro">I. Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Sketching is a powerful form of abstraction that captures the essence of objects using minimal visual elements. 
            This process requires selecting key visual features that convey meaning while omitting others, which demands a level of semantic understanding that is more difficult for machines than for humans. 
            Recent advances, such as CLIPasso \cite{vinker2022clipasso}, have enabled machines to automatically convert images into recognizable abstract sketches using semantic information utilizing SOTA image-text representation method Contrastive Language–Image Pretraining (CLIP) \cite{CLIP}. 
            However, these methods are currently limited to two dimensions. 
          </p>
          <p>
            In this project, we try to extend this idea to 3D: can machines generate 3D sketches that preserve both semantic and geometric information of an object? 
            Addressing this question could open up new possibilities for visual communication and rapid prototyping, paving the way for innovative tools in 3D design and visualization.
          </p>
        </div>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>



<!------------------------------------------------------------------------------RELATED WORK----------------------------------------------------------------------------->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Related work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="related-work">II. Related Works</h2>
        <div class="content has-text-justified">
          <p>
            The intersection of 3D sketch abstraction and differentiable rendering is a vibrant area of research. Our project builds directly on two complementary threads: semantically-driven sketch generation using CLIP, and efficient, real-time rendering through Gaussian primitives. Below, we outline key prior work in both directions that inform our method.
          </p>
        </div>
      </div>
    </div>
  </div>
  <br>

  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- CLIPasso -->
      <div class="column">
        <div class="content">
          <h2 class="title is-6">CLIPasso: Semantically-Aware Object Sketching
                                 <a href="https://clipasso.github.io/clipasso/">[1]</a></h2>
          <p>
            CLIPasso introduced a novel sketch abstraction pipeline driven by CLIP embeddings. It optimizes 2D Bézier curves to semantically match target images, generating vector sketches that maintain both recognizability and abstraction. CLIP-based supervision enables sketching without pixel-wise supervision, focusing instead on perceptual similarity.
          </p>
          <div class="box">
            <img src="static/website_assets/related_1.gif"
                 alt="CLIPasso vector sketch abstraction.">
            <h5 class="subtitle has-text-centered">
              <font size="-1.5">
                <span style="font-weight:normal">Sketches of objects obtained from 2D images using CLIPasso.</span>
              </font>
            </h5>
          </div>
          <p>
            However, CLIPasso operates purely in 2D and does not generalize to multiview-consistent or 3D-aware sketching tasks.
          </p>
        </div>
      </div>
      <!--/ CLIPasso -->

      <!-- Gaussian Splatting -->
      <div class="column">
        <div class="content">
          <h2 class="title is-6">3D Gaussian Splatting for Real-Time Radiance Field Rendering
                                 <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">[2]</a></h2>
          <p>
            This work replaces dense volumetric representations with a sparse set of view-adaptive anisotropic Gaussians, achieving high-quality and real-time radiance field rendering. The technique offers a fully differentiable rendering pipeline ideal for interactive and fast applications.
          </p>
          <div class="box">
            <img src="static/website_assets/related_2.gif"
                 alt="CLIPasso vector sketch abstraction.">
            <h5 class="subtitle has-text-centered">
              <font size="-1.5">
                <span style="font-weight:normal">Representing 3D scenes using Gaussians</span>
              </font>
            </h5>
          </div>
          <p>
            While highly efficient, this work focuses on photorealistic synthesis and lacks semantic constraints. Our work adapts its rasterization foundation for use with sparse Bézier-based sketches guided by perceptual objectives.
          </p>
        </div>
      </div>
    </div>
  </div>
  <br>

  <div class="container is-max-desktop">
    <!-- Conclusion of Related Work -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Combining the semantic sketching approach of CLIPasso with the real-time rendering capabilities of Gaussian Splatting, our framework introduces a differentiable 3D sketch representation optimized using CLIP-based supervision. This positions our work uniquely at the intersection of 3D vision, neural rendering, and semantic abstraction.
          </p>
        </div>
      </div>
    </div>
    <!--/ Conclusion -->
  </div>
</section>


  
<!------------------------------------------------------------------------------METHODOLOGY----------------------------------------------------------------------------->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="method">III. Methodology</h2>
        <div class="content has-text-justified">
          <p>
            In this section, we present the methodology utilized for this paper. We first discuss the environment setup,
            after which we detail how the predator and prey agents are created. Finally, we talk about how the agents are optimized.
          </p>
          <p>
            We create a 3D environment using the Unity game development engine <a href="https://unity.com/">[3]</a>. The selection of Unity as the engine
            for creating the environment instead of a different simulator is motivated by several key factors. Mainly, Unity's ML-Agents
            package <a href="https://arxiv.org/abs/1809.02627">[4]</a> offers support for the OpenAI Gym framework
            <a href="https://arxiv.org/abs/1809.02627">[5]</a>, which is widely used in
            reinforcement learning research. This way, Unity enables interaction between the environment and the learning algorithms,
            allowing for efficient experimentation and evaluation of predator-prey agents.
          </p>
          <p>
            Moreover, the engine offers a wide range of built-in tools, such as physics simulations, ray tracing, and collision.
            Finally, Unity provides a user-friendly and intuitive development environment, making it accessible to researchers with
             varying levels of expertise.
          </p>
        </div>
        <!-- Creating environments. -->

        
<!------------------------------------------------------------------------------SPHERICAL GAUSSIANS----------------------------------------------------------------------------->
        
<h3 class="title is-4">III-A. 3D Differentiable Rasterization with Spherical Gaussians</h3>
<div class="content has-text-justified">
  <p>
    To represent 3D sketches in a differentiable manner, we model 3D Bézier curves using spherical Gaussians (SGs) sampled along each curve’s path. This design allows efficient rasterization of the sketches into 2D images for training. Unlike CLIPasso’s discrete 2D rasterization, our approach supports backpropagation in 3D space and enables full differentiability with respect to the control points. Inspired by recent advances in Gaussian Splatting, we represent Bézier curves as sequences of Gaussians, each defined by a center and a fixed thickness, that can be projected onto 2D views using differentiable camera models.
  </p>

  <div class="columns is-centered">
    <div class="column is-three-quarters">
      <div align="center" class="box">
        <img src="static/website_assets/image_2.png" alt="3D Bezier curves are used for representing 3D sketches of objects.">
        <h5 class="subtitle has-text-centered">
          <font size="-0.7">
            <span style="font-weight:normal">
              3D Bézier curves are used for representing 3D sketches of objects.
            </span>
          </font>
        </h5>
      </div>
    </div>
  </div>

  <p>
    Each spherical Gaussian is defined as:
  </p>
  <p>
    <script type="math/tex; mode=display">
      \phi\left(\mathbf{x}; \mathbf{c}, r\right) = \exp\left(-\frac{\|\mathbf{x} - \mathbf{c}\|^2}{2r^2}\right)
    </script>
  </p>

  <p>
    where <script type="math/tex">\mathbf{c} \in \mathbb{R}^3</script> is the center of the Gaussian and <script type="math/tex">r</script> is its radius (or thickness). To sample a Bézier curve of length <script type="math/tex">L</script> with a desired overlap ratio <script type="math/tex">\alpha_o</script>, the sampling step <script type="math/tex">d</script> and the number of samples <script type="math/tex">N</script> are computed as:
  </p>

  <p>
    <script type="math/tex; mode=display">
      d = r \alpha_o, \quad
      N = \left\lceil \frac{L}{d} \right\rceil
    </script>
  </p>

  <p>
    For a Bézier curve <script type="math/tex">\mathbf{B}</script> with control points <script type="math/tex">\mathbf{p}_0, \mathbf{p}_1, \ldots, \mathbf{p}_M \in \mathbb{R}^3</script>, the centers of the Gaussians sampled along the curve are given by:
  </p>

  <p>
    <script type="math/tex; mode=display">
      \mathbf{c}_{n,\mathbf{B}} =
      \sum_{i=0}^{M}
      \gamma_i\,\left(1-\frac{n}{N}\right)^{M-i}\,\left(\frac{n}{N}\right)^{i}\,\mathbf{p}_i
    </script>
  </p>

  <p>
    where <script type="math/tex">\gamma_i</script> are the Bernstein basis coefficients. This makes each Gaussian center fully differentiable with respect to the Bézier control points.
  </p>

  <div class="columns is-centered">
    <div class="column is-three-quarters">
      <div align="center" class="box">
        <img src="static/website_assets/image_3.png" alt="Spherical Gaussians are utilized for differentiable rasterization of the 3D sketches.">
        <h5 class="subtitle has-text-centered">
          <font size="-0.7">
            <span style="font-weight:normal">
              Spherical Gaussians are utilized for differentiable rasterization of the 3D sketches.
            </span>
          </font>
        </h5>
      </div>
    </div>
  </div>

  <p>
    After generating the Gaussians, we render them through a differentiable rasterizer that projects the 3D Gaussians into 2D using camera intrinsics and extrinsics. The resulting image <script type="math/tex">\mathbf{V} \in \mathbb{R}^{W \times H}</script> is given by:
  </p>

  <p>
    <script type="math/tex; mode=display">
      \mathbf{V} = T\left(\mathbf{c}, r, \mathbf{In}, \mathbf{Ex}, W, H\right)
    </script>
  </p>

  <p>
    where <script type="math/tex">\mathbf{In}</script> and <script type="math/tex">\mathbf{Ex}</script> denote the intrinsic and extrinsic camera matrices, and <script type="math/tex">W, H</script> define the image resolution. Each pixel <script type="math/tex">\mathbf{V}_{n_1,n_2}</script> can be written as:
  </p>

  <p>
    <script type="math/tex; mode=display">
      \mathbf{V}_{n_1,n_2} = T\left(f(\mathbf{p}_i, \ldots), \ldots\right)
    </script>
  </p>

  <p>
    This end-to-end differentiability allows training the sketch representation directly using image-level loss functions such as CLIP similarity. It was one of the key milestones proposed for the project.
  </p>
</div>
        <!--/ Creating environments. -->

<!------------------------------------------------------------------------------CLIP LOSS----------------------------------------------------------------------------->

  <h3 class="title is-4">III-B. CLIP-based Objective</h3>
<div class="content has-text-justified">
  <p>
    To guide the optimization of the 3D Bézier sketch toward both geometrically accurate and semantically meaningful representations, we leverage a CLIP-based loss function. This loss encourages the rendered sketch views to match the real images not just pixel-wise but in high-level perceptual and semantic space. Following the strategy of CLIPasso, we define a dual-component objective that combines semantic similarity and geometric alignment, each captured from different layers of a pretrained CLIP model.
  </p>

  <p>
    The final loss is a weighted sum of the semantic and geometric components:
  </p>

  <p>
    <script type="math/tex; mode=display">
      \mathcal{L} = w_s \,\mathcal{L}_{\mathrm{semantic}}
           + w_g \,\mathcal{L}_{\mathrm{geometric}}
    </script>
  </p>

  <p>
    Here, <script type="math/tex">w_s</script> and <script type="math/tex">w_g</script> are hyperparameters that balance the contribution of each term. The semantic loss <script type="math/tex">\mathcal{L}_{\mathrm{semantic}}</script> is computed using the output of CLIP’s final fully connected layer, which captures the global alignment between rendered sketches and their corresponding RGB images in CLIP’s joint vision-language embedding space.
  </p>

  <p>
    The geometric loss <script type="math/tex">\mathcal{L}_{\mathrm{geometric}}</script> captures local spatial structure and can be extracted either from CLIP’s early convolutional layers or from token-wise attention maps, depending on the selected variant. This term ensures that the layout and fine-scale details of the projected sketches align with real-world object contours.
  </p>

  <p>
    During training, the 3D sketch is projected into 2D using the differentiable rasterizer described in Section III-A. The resulting image is then passed through CLIP alongside its target RGB counterpart. The cosine distance between their embeddings drives the optimization, gradually refining the control points of the Bézier curves so that the rendered sketches become increasingly perceptually and semantically aligned with real views.
  </p>
</div>
<!------------------------------------------------------------------------------TRAINING----------------------------------------------------------------------------->
        <h3 class="title is-4">III-C. Model Training</h3>
        <div class="content has-text-justified">
          <p>
            The training phase aims to optimize a set of 3D Bézier curves such that their 2D projections resemble real object images when viewed from different camera angles. This is achieved through an iterative loop involving projection, perceptual comparison, and gradient-based refinement. By leveraging CLIP embeddings as a supervisory signal, the model learns to align the rendered sketch views with the semantics of real RGB views. The training continues until convergence, producing a geometry-aware sketch representation that is both visually and semantically faithful.
          </p>
            <div class="columns is-centered">
            <div class="column is-three-quarters">
              <div align="center" class="box">
                <img src="static/website_assets/method_1.gif" alt="Rasterization of images using camera matrix from the constructred 3D sketch.">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Rasterization of images using camera matrix from the constructred 3D sketch.</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>
          
          <p>
In each training iteration, the current 3D Bézier sketch is projected into 2D using the known camera intrinsics and extrinsics. This rasterization step generates a view-dependent binary sketch image that mimics how the 3D curves would appear from that specific viewpoint.          
          </p>
          
          <div class="columns is-centered">
            <div class="column is-three-quarters">
              <div align="center" class="box">
                <img src="static/website_assets/method_2.gif" alt="Obtaining the CLIP Loss between the RGB images of the views and sampled sketch views.">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Obtaining the CLIP Loss between the RGB images of the views and sampled sketch views.</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>

          <p>
  The projected sketch view is then compared to the corresponding RGB image of the same view. Both are encoded using CLIP’s vision-language model, and a cosine similarity loss is computed to quantify their perceptual alignment in the embedding space.
          </p>

          <div class="columns is-centered">
            <div class="column is-three-quarters">
              <div align="center" class="box">
                <img src="static/website_assets/method_3.gif" alt="Optimizing the control points using the backpropagated loss.">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Optimizing the control points using the backpropagated loss.</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>

          <p>
  The CLIP loss is backpropagated through the differentiable projection and rendering pipeline to adjust the 3D control points of the Bézier curves. This step ensures that the curves evolve to better match the visual content of the reference images.
          </p>

          <div class="columns is-centered">
            <div class="column is-three-quarters">
              <div align="center" class="box">
                <img src="static/website_assets/method_4.gif" alt="Training the model until loss converges.">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Training the model until loss converges.</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>

           <p>
  This process is repeated iteratively for all views in the dataset. Over time, the loss steadily decreases as the curves converge to a semantically accurate and geometrically coherent representation of the object across all viewpoints.
          </p>

          
        </div>
        <!--/ Creating environments. -->
      </div>
    </div>
  </div>
</section>


<!------------------------------------------------------------------------------EXPERIMENTS & RESULTS----------------------------------------------------------------------------->


  <section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="results">IV. Results</h2>

        <p>
          In this section, we discuss the setup and results of the experiments in detail. Regarding results, we first discuss the training progression, after which we do both a qualitative and
          a quantitative analysis of the different agent configurations.
        </p>
        <br>

        <!-- Inference environments. -->
        <h3 class="title is-4">IV-A. Experimental Setup</h3>

        <br/>
        <div class="content has-text-justified">
          <div class="container">
            <!-- <h2 class="title is-2 has-text-centered">Model and Behavior Parameters</h2> -->
                <h3 class="title is-5">Agent parameters</h3>

                <div class="box">
                  <div class="table-container">
                      <table class="table" align="center">
                          <thead>
                              <tr>
                                  <th></th>
                                  <th>Rays per direction</th>
                                  <th>Depth rays per direction</th>
                                  <th>Max ray degrees</th>
                                  <th>Ray length</th>
                                  <th>Observation stacks</th>
                                  <th>Field of view</th>
                                  <th>Binocular region</th>
                                  <th>Maximum movement speed</th>
                                  <th>Maximum rotation speed</th>
                              </tr>
                          </thead>
                          <tbody>
                              <tr>
                                  <td><b>Prey-style</b></td>
                                  <td>30</td>
                                  <td>4</td>
                                  <td>160</td>
                                  <td>15</td>
                                  <td>5</td>
                                  <td><em>320°</em></td>
                                  <td><em>43°</em></td>
                                  <td><em>8</em></td>
                                  <td><em>8</em></td>
                              </tr>
                              <tr>
                                  <td><b>Predator-style</b></td>
                                  <td>30</td>
                                  <td>20</td>
                                  <td>85</td>
                                  <td>15</td>
                                  <td>5</td>
                                  <td><em>170°</em></td>
                                  <td><em>113°</em></td>
                                  <td><em>6</em></td>
                                  <td><em>2</em></td>
                              </tr>
                          </tbody>
                      </table>
                  </div>
                  <div align="center">
                  <font size="-0.7">
                    <span style="font-weight:normal">
                      Parameters of the prey and predator-style vision sensors, and the behavioural parameters used for the agents.
                  </span>
                  </font>
                  </div>
                </div>

                <p>Firstly, we establish two distinct categories of vision: prey-style and predator-style.
                  This can be seen in the table above.
                  Prey-style vision emphasizes a broad field of view while limiting the binocular region,
                  whereas predator-style vision exhibits the opposite characteristics. Additionally, for both agents we
                  stack the observations to simulate a brief memory of recent events.
                </p>

                <p>
                  During inference, we designate the maximum speed of the prey to be greater
                  than that of the predator. We adopt this approach to give the prey a better chance to play out
                  its learned strategies rather than getting caught too quickly in each episode.

                </p>

                <div class="columns is-centered">
                  <div class="column is-three-fifths">
                    <div class="box">
                      <img src="static/images/sensor_configs.png" alt="Vision sensor configurations">
                      <h5 class="subtitle has-text-centered">
                        <font size="-0.7">
                          <span style="font-weight:normal">
                            The combinations of vision sensors used for the predator and prey in the experiments.
                        </span>
                        </font>
                      </h5>
                    </div>
                  </div>
                </div>

                <p>
                  As depicted in the figure above, we train agents in \(4\) configurations to cover each combination of
                  vision types for the predator and prey. The trained models are then used to perform inferences.
                </p>

          </div>
        </div>

        <div class="content has-text-justified">
          <div class="container">

            <h3 class="title is-5">Training details</h3>
            <p>
              The agents are trained on all environments mentioned in the methodology section in parallel, using the parameters mentioned in the agent parameters section.
              The only difference is that the maximum speed of the prey was set to \(5.5\). By having environments of different difficulties,
              a speed-up in training can be observed, similar as in curriculum learning <a href="https://dl.acm.org/doi/abs/10.1145/1553374.1553380?casa_token=nh2ZrojaWoAAAAAA:ch-UtBUm-jXdt36EJoGrONsdXTfKYTOF9g3wpMqUjykIzeeCv2_d4p2hoGlyuJRGTlWCdZyWmPVmmg">[13]</a>,
              <a href="https://dl.acm.org/doi/abs/10.5555/3455716.3455897">[14]</a>. Furthermore, three agents of each type were present
              in each environment. Although this could make the environment unstable due to partial observability, it greatly enhances training speed.
            </p>
            <p>
              The predator and prey were each trained self-play for \(9 \cdot 10^6\) steps, where one step corresponds to
              one frame in the simulation. The swap between the frozen training teams happens every \(3 \cdot 10^5\) training steps. At
              each episode, the agent is spawned with a random rotation the y-axis, and the maximum episode duration is \(10^4\).
              The model plays against the most recent version of the agent \(80\%\) of the time. The models both consist of
              a \(3\) hidden-layer MLP with \(512\) neurons with batch normalisation at each layer. The model is optimized using
              PPO with the Adam <a href="https://arxiv.org/abs/1412.6980">[15]</a> optimizer with a learning rate of \(0.0003\) and no decay. Finally, the buffer sizes of both
              agents are set to \(40960\).
            </p>

            <h3 class="title is-5">Inference environments</h3>
            <p>
              For testing the agents we design new out of distribution environments with varying levels of complexity. The control
              environment, like in training, consists of an empty environment. Additional environments include similar obstacles and
              hiding spots in different configurations, introducing more blind spots for the prey to utilize for protection. The inference
              environments are shown below.
            </p>

          </div>
        </div>

        <br/>


      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <a href="./static/images/inference/control.jpg" data-lightbox="image">
            <img src="./static/images/inference/control.jpg" alt="Control Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Control</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Control</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/trees.jpg" data-lightbox="image">
            <img src="./static/images/inference/trees.jpg" alt="Trees Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Trees</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Trees</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/rocks.jpg" data-lightbox="image">
            <img src="./static/images/inference/rocks.jpg" alt="Rocks Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Rocks</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Rocks</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/hide.jpg" data-lightbox="image">
            <img src="./static/images/inference/hide.jpg" alt="Hide Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Hide</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Hide</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/seek.jpg" data-lightbox="image">
            <img src="./static/images/inference/seek.jpg" alt="Seek Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Seek</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Seek</p>
          </div>
        </div>
      </div>
    </div>
    <h6 class="subtitle has-text-centered">
      Environments used for inference
    </h6>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <div class="content has-text-justified">
          <div class="container">
            <h3 class="title is-5">Hardware</h3>
            <p>
              The models were trained using a CUDA-enabled NVIDIA
              V100 PCIe \(32\) GB GPU with \(7\) TFLOPS, a Xeon-Gold processor running with a \(2.1\) GHz clock speed, and \(16\) GB of
              RAM. Our method is implemented with Unity’s ML-Agents package and PyTorch, running on Linux with Python 3.7.
            </p>

          </div>
        </div>

      </div>
    </div>
    <!--/ Experiments. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Results. -->
        <!-- Include median_time_survived.html using an iframe -->
        <h3 class="title is-4">IV-B. Results</h3>
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <div class="container">
              <!-- <h2 class="title is-2 has-text-centered">Model and Behavior Parameters</h2> -->
                  <h3 class="title is-5">Training results</h3>
                  <div class="box">
                      <img src="./static/images/training-plot.png" alt="Reward plot during training">
                      <h5 class="subtitle has-text-centered">
                        <font size="-0.7">
                          <span style="font-weight:normal">
                            <b>(Left)</b> Average returns over \(9\) million steps of training for each agent configuration.
                            <b>(Right)</b> Average episode lengths over \(9\) million steps of training for each agent configuration.
                            Note that, unfortunately, the training logs for 'Normal' vision were lost. However the trend in rewards
                            was similar to the other plots shown here. Furthermore, take note that the x-axis represents the number of steps,
                            so if the predator was trained first, it would swap at the step \(300000\), and the x-label for the prey at total step \(300005\)
                            would be \(5\). An exponential smoothing of strength \(0.8\) is used.
                        </span>
                        </font>
                      </h5>
                  </div>
                  <p>
                    The average returns during training for various vision configurations of predator and prey agents are illustrated in the left figure. It is
                    observed that the PredatorBoth vision improves at a slower rate compared to other configurations, likely due to the reduced field of
                    view when both agents have predator vision. However, this effect diminishes over time.
                  </p>
                  <p>
                    In the right figure, we analyze the episode length throughout the training process. Interestingly, we consistently observe a decreasing trend in episode
                    length across all configurations. It is important to note however that whilst it may seem like the predator is learning to consistenly dominate the
                    prey, the training environment is heavily biased towards the predator since there are three predators which can all catch the prey. Thus, to judge
                    the true performance of the agents we perform inference in a more balanced environment.
                    </thead>
                  </p>
                  <br>
            </div>


            <div class="container">
              <!-- <h2 class="title is-2 has-text-centered">Model and Behavior Parameters</h2> -->
                <h3 class="title is-5">Inference results</h3>
                <p>
                The plot below shows median number of steps survived by the prey in the inference environments for
                different configurations of vision sensors. We look at time survived in order to study the relative
                performance of each agent in different settings (higher time indicates more effective prey, lower time indicates more
                effective predator). We use median so that the results are not swayed by outliers which might be caused due to
                no interactions between the predator and prey, for example if they get stuck in an obstacle for an episode.
                </p>
                <div class="box">
                  <iframe src="./static/figures/median_time_survived.html" frameborder="0" width="100%" height="500px"></iframe>
                </div>

                <br>
                <p>
                To comment further on the learned strategies of the agents under different configurations and to identify the
                reasons for the results we see above, we qualitatively assess some key behaviour patterns of the trained agents
                in each configuration.
                </p>
                <h3 class="title is-6">Normal vision</h3>
                <p>
                  This configuration features agents that are trained with their typical real-world vision field. We observe
                  many instances that suggest both agents have learnt strategies to maximize their reward.
                  The prey performs significantly worse with this configuration in the control environment, suggesting that it has learnt to
                  effectively use obstacles in the other environments to evade the predator.
                </p>
                <p>
                  <em>Predator:</em> Chases the prey while trying to keep it in its binocular region. When the prey is in the monocular region
                  it seems to favour rotation with slower movement speed, but when the prey is in its binocular region it makes bigger steps towards
                  the prey. This likely hints at the importance of depth information for the predator, but it's possible that this is also influenced by the fact
                  that the agents have very little memory (as evidenced by the predator forgetting about the prey as it falls out of its vision field),
                  and is thus trying to simply keep the prey within its vision field for as long as possible.
                </p>
                <p>
                  <em>Prey:</em> As seen below, it has visibly learnt ways to evade the predators. It is able to keep track of the
                  predator using its monocular vision  and also uses obstacles to its advantage
                  by either hiding behind or going around them. It also uses a zig zag motion to avoid the predator
                  being in its blind spot.
                </p>
            </div>
          </div>
        </div>


        <!--/ Results. -->

      </div>
    </div>
    <!--/ Experiments. -->
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" controls playsinline height="100%">
            <source src="./static/videos/Normal/Trees1.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey evading the predator using the trees. Predator forgetting about the prey.
          </h2>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" controls playsinline height="100%">
            <source src="./static/videos/Normal/hiding.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey hiding behind the rock unseen by the predator
          </h2>
        </br>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" controls playsinline height="100%">
            <source src="./static/videos/Normal/zig-zag.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey exhibitsing zig-zag movement when the predator is behind
          </h2>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" controls playsinline height="100%">
            <source src="./static/videos/Normal/prey-bivsmonovison-behaviour-control.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Different behaviours when the prey is in the binocular/monocular FOV of the predator
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Results. -->
        <!-- Include median_time_survived.html using an iframe -->
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <h3 class="title is-6">Both predator</h3>
            <p>
              Since the prey is trained using predator-style vision, its blind spot is much larger, thus the prey is unaware if
              it is being chased from behind. It can only rely on cues ahead of it to keep track of objects and the predator.
            </p>
            <p>
              Overall in this configuration there are also many instances which show that the prey has failed to properly learn to evade,
              even when the predator is directly in its vision field, indicating that these agents likely require further training.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" controls playsinline height="100%">
            <source src="./static/videos/PredatorBoth/Control.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey failed to learn to effectively evade predator
          </h2>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" controls playsinline height="100%">
            <source src="./static/videos/PredatorBoth/trees.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey cannot react to predator approaching from behind (blind spot)
          </h2>
        </br>
        </div>
        <div class="item item-shiba">
          <video poster="" id="chair-tp" controls playsinline height="100%">
            <source src="./static/videos/PredatorBoth/not_trained_enough.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey movement indicating that further training is required
          </h2>
        </div>
        <div class="item item-chair-tp">
        </div>
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Results. -->
        <!-- Include median_time_survived.html using an iframe -->
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <h3 class="title is-6">Both prey</h3>
            <p>
              The median time survived by the prey is significantly higher across all environments in this configuration,
              indicating that the predator trained with prey-style vision is likely ineffective, whilst the prey is able to
              learn effetively.
            </p>
            <p>
              <em>Predator:</em> Exhibits less aggressive behaviour and more tracking(in order to keep the prey in its narrow
              depth FOV) by rotating when the prey is in its monocular
              vision. The lower aggression exhbited by the predator is a major reason for longer median survival time
              of the prey.
            </p>
            <p>
              <em>Prey:</em> We observe that the prey has
              learnt evasive strategies similar to the prey in the normal configuration. This can be
              seen in the video of the prey evading the predator in the control environment.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" controls playsinline height="100%">
            <source src="./static/videos/PreyBoth/Control.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey both performing well in the control environment
          </h2>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" controls playsinline height="100%">
            <source src="./static/videos/PreyBoth/rotation.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Rotational behaviour preferred by agent when the adversary is in the monocular FOV
          </h2>
        </br>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" controls playsinline height="100%">
            <source src="./static/videos/PreyBoth/Trees.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey exhibiting evasive maneuvers
          </h2>
        </div>
        <div class="item item-fullbody">
        </div>
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Results. -->
        <!-- Include median_time_survived.html using an iframe -->
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <h3 class="title is-6">Swapped vision</h3>
            <p>
              This configuration has lower median time survived for the prey in all environments except control.
            </p>
            <p>
              <em>Prey:</em> Exhibits behaviours suggesting that it has learnt to keep track of the predator in its frontal vision field
              and move backwards to run away from the predator. However since it is lacking a wide field of view, it is unware of the
              objects behind it and runs into them, making it an easy target. Thus the prey performs worse in environments with many
              obstacles (such as trees) and better in control which has no obstacles.
              In many examples it keeps backing up into the walls and corners until it is eventually caught
              by the predator. This is likely due to a lack of memory and positional information, which means that the agent doesn't realise that it's
              actions are futile and it's not moving anywhere.
            </p>
            <p>
              <em>Predator:</em> Since it is trained on prey-style vision, it continues to rotate to try keep the prey in its binocular region,
              similar to the both-prey configuration. However since the prey is also less effective with predator-style vision, the predator
              has more success in this case with the median survival time for the prey being lower than the both-prey case (except for in control where the
              prey can be effective with predator-style vision).
            </p>
            <p>

            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" controls playsinline height="100%">
            <source src="./static/videos/Swapped/Trees.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey learns to evade by keep predator in the depth FOV
          </h2>
        </div>

        <div class="item item-shiba">
          <video poster="" id="shiba" controls playsinline height="100%">
            <source src="./static/videos/Swapped/backing-into-corner.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey backs up into a corner trying to evade the predator
          </h2>
        </br>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" controls playsinline height="100%">
            <source src="./static/videos/Swapped/Control-lasts-longer.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey surviving longer in the control environment due to lack of obstacles
          </h2>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" controls playsinline height="100%">
            <source src="./static/videos/Swapped/backing-into-wall.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey backing up into the wall lacking memory and perception behind it
          </h2>
        </div>

          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!------------------------------------------------------------------------------CONC  & LIMITATIONS----------------------------------------------------------------------------->

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Conclusions and limitations. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="conclusion">V. Conclusion and Limitations</h2>

        <div class="content has-text-justified">
          <p>
            Our work explores the use of CLIP-based objectives for 3D sketch abstraction via differentiable Bézier curves. Through our implementation and experimentation, we make the following observations:
            <ul>
              <li> Depth ambiguity in single-view optimization leads to visually plausible but geometrically inconsistent 3D reconstructions.
              <li> Multi-view supervision stabilizes depth inference and leads to spatially coherent sketches across viewpoints.
              <li> Differentiable Gaussian rasterization enables gradient-based optimization over curve control points, but memory efficiency is critical to scale.
              <li> The 3D sketch generation task can be reduced to a Gaussian Splatting problem with alpha, color and variance parameters are fixed and mean values of the splats are subject to constraints as Beizer Curve control points:
            </ul>

            These findings highlight the importance of viewpoint diversity and geometry-aware initialization in abstract 3D reconstruction tasks, and support the use of CLIP-based semantic loss as a viable surrogate when explicit 3D supervision is unavailable.
            </p>
        </div>

        <!-- Limitations. -->
        <h3 class="title is-4">V-A. Limitations and Future Work</h3>
        <div class="content has-text-justified">
          <p>
            Although we have obtained promising results, there are several limitations in our work that could be addressed in future research:
          </p>
          <ul>
            <li>
              Multi-view consistency remains one of the most critical and challenging objectives in the field of 3D image generation and editing. In our work, despite employing batches of multi-view images and experimenting with various hyperparameter settings, we observed failures in achieving semantic consistency across views for certain objects.
              Notably, we found that the CLIP-based loss occasionally overemphasizes semantic similarity at the expense of geometric coherence. For example, in the case of the horse model, the object may resemble a full horse from one viewpoint but appear as only a horse’s head from another. A similar inconsistency is evident in the GIF below, where the bicycle sketch exhibits view-dependent deformation appearent from the tires.
              Addressing this issue requires further refinement of the training strategy—particularly in the initialization and optimization of Bézier control points. Additionally, a more careful tuning of the CLIP loss weighting parameters may help balance semantic alignment with geometric structure across views.
            </li>
          </ul>

          <div class="columns is-centered">
            <div class="column is-half">
              <div align="center" class="box">
                <img src="static/results/bike/14.05 - bike - efe/semihelical.gif" alt="The bicycle sketch exhibits view-dependent deformation apparent from the tires">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal"><em>The bicycle reconstruction exhibits view-dependent deformation apparent from the tires</em></span>
                  </font>
                </h5>
              </div>
            </div>
          </div>

          <ul>
            <li>
               Another challenge encountered during training is the tendency of the optimization process to overfill the volume of the object when the number of Bézier curves becomes too high. As the sketch density increases, the CLIP-based loss drives the model to generate a solid, filled-in representation rather than preserving the sparse, abstract nature of a line-based sketch.
              This behavior may be partially related to the previously discussed issue, where the CLIP loss struggles to maintain a proper balance between semantic and geometric alignment across views. However, it also appears to be influenced by the number of sketch curves: as more curves are introduced, the optimization increasingly prioritizes semantic coverage, effectively “filling in” the shape.
              This effect is illustrated in the duck reconstruction GIF below, where the sketch—composed of 40 curves—visibly fills the object’s interior volume rather than outlining its structural features. 

<div class="columns is-centered">
            <div class="column is-half">
              <div align="center" class="box">
                <img src="static/results/duck/18.05 - duck_2 - efe/semihelical.gif" alt="The duck reconstruction exhibits a problem of volume-filling when number of lines to model the sketch is increased.">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal"><em>The duck reconstruction exhibits a problem of volume-filling when number of lines to model the sketch is increased.</em></span>
                  </font>
                </h5>
              </div>
            </div>
          </div>

              To address this problem, additional geometric constraints on curve proximity could be introduced to discourage excessive overlap. 
              Alternatively, further improvements to the semantic-geometric balance described earlier may also alleviate this behavior. 
              Exploring dynamic loss weighting strategies or incorporating structural priors remains a promising direction for future work.
            </li>
          </ul>

          <ul>
            <li>
The initialization strategy for Bézier curves plays a critical role in achieving high-quality sketch reconstruction. In CLIPasso, curve initialization is guided by thresholding saliency maps produced by the CLIP model, enabling semantically meaningful placements in 2D. However, this approach does not generalize well to 3D, where surface geometry must be taken into account. To keep the scope of the project focused and tractable, we chose not to train a NeRF or Gaussian Splatting model to obtain 3D saliency maps for curve placement.

Instead, we adopted a random initialization strategy, which, despite its simplicity, yielded reasonably good results. Nevertheless, we believe that the visual quality and consistency of the generated sketches could be significantly improved with more informed initialization schemes—particularly those that incorporate geometric priors or 3D-aware saliency cues.            </li>
          </ul>

          <p>
            In this project, we developed a novel sketch generation technique built upon the principles of Gaussian splatting, enhanced with task-specific constraints. While the visual quality of our results may fall short compared to state-of-the-art methods in the literature, the framework and research introduced here offer strong potential for reuse and extension. In particular, our approach provides a foundation for future academic work focused on task-oriented, constrained 3D applications—an area that remains largely underexplored and rich with opportunity.
          </p>

        </div>
        <br/>
        <!--/ Limitations and future work-->

      </div>
    </div>
    <!--/ Conclusions and limitations. -->
  </div>
</section>



<!------------------------------------------------------------------------------CONTRIBUTIONS---------------------------------------------------------------->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- Optional icon link (e.g., to GitHub repo) -->
      <a class="icon-link" href="https://github.com/tarhanefe/clipasso3d" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>

    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <h6 class="title is-6">Individual Contributions</h6>
          <p>
            E.T. developed and implemented the 3D spherical Gaussian splatting structure and the differentiable rasterization algorithms.
            E.T. also designed the 3D training scheme and ran experiments on both single-view and multi-view datasets.
            A.R. contributed by evaluating the rendering pipeline with multi-view data.
            F.M. implemented saliency map computation and constructed the dataset generation pipeline using Blender.
            E.I. assisted in implementing differentiable rasterization, refining the training loop, and setting up camera sampling.
          </p>
          <p><em>
            This website is based on the <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies website template</a>,
            which is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </em></p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
